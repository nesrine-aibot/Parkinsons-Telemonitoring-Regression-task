# -*- coding: utf-8 -*-
"""parkinsons-8model-ext.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NGyuPVgtG3GycuT_7K79-IzQXzw--NxY
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.applications import InceptionV3, VGG16, MobileNetV2, Xception, NASNetLarge, DenseNet201, DenseNet121
from sklearn.metrics import mean_squared_error

import pandas as pd

file_path = r"/content/sample_data/parkinsons_updrs.data"
df = pd.read_csv(file_path)
print(df.head())

# Separate features (X) and target (y)
X = df.drop(['total_UPDRS'], axis=1)  # Adjust 'target_column_name' to your target column name
y = df['total_UPDRS']
X, y

scaler = StandardScaler()

# Fit and transform the features (X)
X_scaled = scaler.fit_transform(X)

# Now X_scaled contains the scaled features, and y contains the target values
print("Shape of X_scaled:", X_scaled.shape)
print("Shape of y:", y.shape)

X_scaled.shape

import numpy as np
from sklearn.preprocessing import StandardScaler

# Assuming X_scaled is (5875, n_features)
num_samples = X_scaled.shape[0]
num_features = X_scaled.shape[1]
desired_shape = (66, 66, 3)
desired_flat_size = np.prod(desired_shape)  # 13068

# Pad each row with zeros to reach 13068 features
if num_features > desired_flat_size:
    raise ValueError("Your feature size is too big to reshape into desired image shape.")

# Pad with zeros at the end of each row
X_padded = np.pad(X_scaled, ((0, 0), (0, desired_flat_size - num_features)))

# Reshape into image format
X_image = X_padded.reshape(num_samples, *desired_shape)
print("New shape:", X_image.shape)

from skimage.transform import resize

# Desired final shape
desired_height = 75
desired_width = 75

# Resize each sample
X_resized = np.zeros((X_image.shape[0], desired_height, desired_width, 1))

for i in range(X_image.shape[0]):
    X_resized[i] = resize(X_image[i], (desired_height, desired_width, 1), anti_aliasing=True)

print("New resized shape:", X_resized.shape)

def reshape_split(X, y, dim):
    desired_size = dim - (dim % X.shape[1])
    num_channels = 3

    # Calculate the number of repeats needed to match the desired size
    num_repeats = (desired_size * desired_size * num_channels) // X_scaled.shape[1]

    # Reshape and upsample the scaled input data to match the desired shape
    X_reshaped = np.repeat(X_scaled[:, np.newaxis, :], num_repeats, axis=1)
    X_reshaped = X_reshaped.reshape(X_scaled.shape[0], desired_size, desired_size, num_channels)

    print("Shape of X_reshaped:", X_reshaped.shape)

    desired_size = dim
    # Create a new array of zeros with the desired shape
    new_shape = (X_reshaped.shape[0], desired_size, desired_size, X_reshaped.shape[3])
    X_reshaped_padded = np.zeros(new_shape)

    # Calculate the starting indices to copy the original data into the padded array
    start_row = (desired_size - X_reshaped.shape[1]) // 2
    start_col = (desired_size - X_reshaped.shape[2]) // 2

    # Copy the original data into the padded array
    X_reshaped_padded[:, start_row:start_row+X_reshaped.shape[1], start_col:start_col+X_reshaped.shape[2], :] = X_reshaped

    print("Shape of X_reshaped_padded:", X_reshaped_padded.shape)

    # Split the reshaped and scaled data into training and testing sets
    X_train_final, X_test_final, y_train, y_test = train_test_split(
        X_reshaped_padded, y, test_size=0.2, random_state=42
    )

    print("Shape of X_train_final:", X_train_final.shape)
    print("Shape of X_test_final:", X_test_final.shape)
    print("Shape of y_train:", y_train.shape)
    print("Shape of y_test:", y_test.shape)

    return X_train_final, X_test_final, y_train, y_test

X_train_final, X_test_final, y_train, y_test = reshape_split(X, y, 75)

def f1(p, r):
    return (2 * p * r) / (p+r)

import time
start = time.time()
end = time.time()
print(f"Computational time {end - start} seconds")

import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
start = time.time()

# Load the pre-trained VGG16 model (excluding the top layers)
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(75, 75, 3))

# Freeze the base model layers
base_model.trainable = False

# Build the regression model on top of the base model
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(16, activation='relu'),
    Dense(4, activation='relu'),
    Dense(1, activation='linear')  # Linear activation for regression
])

# Define the desired learning rate (e.g., 0.01)
learning_rate = 0.01

# Compile the model with the specified learning rate and mean_squared_error loss
optimizer = Adam(learning_rate=learning_rate)
model.compile(
    optimizer=optimizer,
    loss='mean_squared_error',  # Change to regression loss
    metrics=[
        'mae',  # Mean Absolute Error
        'mse'   # Mean Squared Error
    ]
)
# Display a summary of the model architecture
model.summary()

# Train the model
history = model.fit(X_train_final, y_train, epochs=8, batch_size=32, validation_split=0.1)

# Evaluate the model on the test set
loss, mae, mse = model.evaluate(X_test_final, y_test)
print("Mean Squared Error on Test Set:", mse)
print("Mean Absolute Error on Test Set:", mae)
print("Root Mean Squared Error on Test Set:", np.sqrt(mse))

end = time.time()
print(f"Computational time {end - start} seconds")

import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
start = time.time()

base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(75, 75, 3))
base_model.trainable = False

# Build the regression model on top of the base model
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(16, activation='relu'),
    Dense(4, activation='relu'),
    Dense(1, activation='linear')  # Linear activation for regression
])

learning_rate = 0.01
optimizer = Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae', 'mse'])

model.summary()

history = model.fit(X_train_final, y_train, epochs=8, batch_size=32, validation_split=0.1)
loss, mae, mse = model.evaluate(X_test_final, y_test)
print("Mean Squared Error on Test Set:", mse)
print("Mean Absolute Error on Test Set:", mae)
print("Root Mean Squared Error on Test Set:", np.sqrt(mse))

end = time.time()
print(f"Computational time {end - start} seconds")

import tensorflow as tf
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
start = time.time()

base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(75, 75, 3))
base_model.trainable = False
# Build the regression model on top of the base model
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(16, activation='relu'),
    Dense(4, activation='relu'),
    Dense(1, activation='linear')  # Linear activation for regression
])

# Define the desired learning rate (e.g., 0.01)
learning_rate = 0.01

# Compile the model with the specified learning rate and mean_squared_error loss
optimizer = Adam(learning_rate=learning_rate)
model.compile(
    optimizer=optimizer,
    loss='mean_squared_error',  # Change to regression loss
    metrics=[
        'mae',  # Mean Absolute Error
        'mse'   # Mean Squared Error
    ]
)
# Display a summary of the model architecture
model.summary()

# Train the model
history = model.fit(X_train_final, y_train, epochs=8, batch_size=32, validation_split=0.1)

# Evaluate the model on the test set
loss, mae, mse = model.evaluate(X_test_final, y_test)
print("Mean Squared Error on Test Set:", mse)
print("Mean Absolute Error on Test Set:", mae)
print("Root Mean Squared Error on Test Set:", np.sqrt(mse))

end = time.time()
print(f"Computational time {end - start} seconds")

import tensorflow as tf
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
start = time.time()

base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(75, 75, 3))
base_model.trainable = False
# Build the regression model on top of the base model
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(16, activation='relu'),
    Dense(4, activation='relu'),
    Dense(1, activation='linear')  # Linear activation for regression
])

# Define the desired learning rate (e.g., 0.01)
learning_rate = 0.01

# Compile the model with the specified learning rate and mean_squared_error loss
optimizer = Adam(learning_rate=learning_rate)
model.compile(
    optimizer=optimizer,
    loss='mean_squared_error',  # Change to regression loss
    metrics=[
        'mae',  # Mean Absolute Error
        'mse'   # Mean Squared Error
    ]
)
# Display a summary of the model architecture
model.summary()

# Train the model
history = model.fit(X_train_final, y_train, epochs=8, batch_size=32, validation_split=0.1)

# Evaluate the model on the test set
loss, mae, mse = model.evaluate(X_test_final, y_test)
print("Mean Squared Error on Test Set:", mse)
print("Mean Absolute Error on Test Set:", mae)
print("Root Mean Squared Error on Test Set:", np.sqrt(mse))

end = time.time()
print(f"Computational time {end - start} seconds")

import tensorflow as tf
from tensorflow.keras.applications import Xception
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
start = time.time()

base_model = Xception(weights='imagenet', include_top=False, input_shape=(75, 75, 3))
base_model.trainable = False
# Build the regression model on top of the base model
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(16, activation='relu'),
    Dense(4, activation='relu'),
    Dense(1, activation='linear')  # Linear activation for regression
])

# Define the desired learning rate (e.g., 0.01)
learning_rate = 0.01

# Compile the model with the specified learning rate and mean_squared_error loss
optimizer = Adam(learning_rate=learning_rate)
model.compile(
    optimizer=optimizer,
    loss='mean_squared_error',  # Change to regression loss
    metrics=[
        'mae',  # Mean Absolute Error
        'mse'   # Mean Squared Error
    ]
)
# Display a summary of the model architecture
model.summary()

# Train the model
history = model.fit(X_train_final, y_train, epochs=8, batch_size=32, validation_split=0.1)

# Evaluate the model on the test set
loss, mae, mse = model.evaluate(X_test_final, y_test)
print("Mean Squared Error on Test Set:", mse)
print("Mean Absolute Error on Test Set:", mae)
print("Root Mean Squared Error on Test Set:", np.sqrt(mse))

end = time.time()
print(f"Computational time {end - start} seconds")

import tensorflow as tf
from tensorflow.keras.applications import NASNetLarge
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
start = time.time()

base_model = NASNetLarge(weights='imagenet', include_top=False, input_shape=(75, 75, 3))
base_model.trainable = False
# Build the regression model on top of the base model
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(16, activation='relu'),
    Dense(4, activation='relu'),
    Dense(1, activation='linear')  # Linear activation for regression
])

# Define the desired learning rate (e.g., 0.01)
learning_rate = 0.01

# Compile the model with the specified learning rate and mean_squared_error loss
optimizer = Adam(learning_rate=learning_rate)
model.compile(
    optimizer=optimizer,
    loss='mean_squared_error',  # Change to regression loss
    metrics=[
        'mae',  # Mean Absolute Error
        'mse'   # Mean Squared Error
    ]
)
# Display a summary of the model architecture
model.summary()

# Train the model
history = model.fit(X_train_final, y_train, epochs=8, batch_size=32, validation_split=0.1)

# Evaluate the model on the test set
loss, mae, mse = model.evaluate(X_test_final, y_test)
print("Mean Squared Error on Test Set:", mse)
print("Mean Absolute Error on Test Set:", mae)
print("Root Mean Squared Error on Test Set:", np.sqrt(mse))

end = time.time()
print(f"Computational time {end - start} seconds")

import tensorflow as tf
from tensorflow.keras.applications import DenseNet201
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
start = time.time()

base_model = DenseNet201(weights='imagenet', include_top=False, input_shape=(75, 75, 3))
base_model.trainable = False
# Build the regression model on top of the base model
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(16, activation='relu'),
    Dense(4, activation='relu'),
    Dense(1, activation='linear')  # Linear activation for regression
])

# Define the desired learning rate (e.g., 0.01)
learning_rate = 0.01

# Compile the model with the specified learning rate and mean_squared_error loss
optimizer = Adam(learning_rate=learning_rate)
model.compile(
    optimizer=optimizer,
    loss='mean_squared_error',  # Change to regression loss
    metrics=[
        'mae',  # Mean Absolute Error
        'mse'   # Mean Squared Error
    ]
)
# Display a summary of the model architecture
model.summary()

# Train the model
history = model.fit(X_train_final, y_train, epochs=8, batch_size=32, validation_split=0.1)

# Evaluate the model on the test set
loss, mae, mse = model.evaluate(X_test_final, y_test)
print("Mean Squared Error on Test Set:", mse)
print("Mean Absolute Error on Test Set:", mae)
print("Root Mean Squared Error on Test Set:", np.sqrt(mse))

end = time.time()
print(f"Computational time {end - start} seconds")

import tensorflow as tf
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
start = time.time()

base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(75, 75, 3))
base_model.trainable = False
# Build the regression model on top of the base model
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(16, activation='relu'),
    Dense(4, activation='relu'),
    Dense(1, activation='linear')  # Linear activation for regression
])

# Define the desired learning rate (e.g., 0.01)
learning_rate = 0.01

# Compile the model with the specified learning rate and mean_squared_error loss
optimizer = Adam(learning_rate=learning_rate)
model.compile(
    optimizer=optimizer,
    loss='mean_squared_error',  # Change to regression loss
    metrics=[
        'mae',  # Mean Absolute Error
        'mse'   # Mean Squared Error
    ]
)
# Display a summary of the model architecture
model.summary()

# Train the model
history = model.fit(X_train_final, y_train, epochs=8, batch_size=32, validation_split=0.1)

# Evaluate the model on the test set
loss, mae, mse = model.evaluate(X_test_final, y_test)
print("Mean Squared Error on Test Set:", mse)
print("Mean Absolute Error on Test Set:", mae)
print("Root Mean Squared Error on Test Set:", np.sqrt(mse))

end = time.time()
print(f"Computational time {end - start} seconds")

import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
start = time.time()

base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(75, 75, 3))
base_model.trainable = False
# Build the regression model on top of the base model
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(16, activation='relu'),
    Dense(4, activation='relu'),
    Dense(1, activation='linear')  # Linear activation for regression
])

# Define the desired learning rate (e.g., 0.01)
learning_rate = 0.01

# Compile the model with the specified learning rate and mean_squared_error loss
optimizer = Adam(learning_rate=learning_rate)
model.compile(
    optimizer=optimizer,
    loss='mean_squared_error',  # Change to regression loss
    metrics=[
        'mae',  # Mean Absolute Error
        'mse'   # Mean Squared Error
    ]
)
# Display a summary of the model architecture
model.summary()

# Train the model
history = model.fit(X_train_final, y_train, epochs=8, batch_size=32, validation_split=0.1)

# Evaluate the model on the test set
loss, mae, mse = model.evaluate(X_test_final, y_test)
print("Mean Squared Error on Test Set:", mse)
print("Mean Absolute Error on Test Set:", mae)
print("Root Mean Squared Error on Test Set:", np.sqrt(mse))

end = time.time()
print(f"Computational time {end - start} seconds")

import tensorflow as tf
from tensorflow.keras.applications import EfficientNetB3
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
start = time.time()

base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(75, 75, 3))
base_model.trainable = False
# Build the regression model on top of the base model
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(16, activation='relu'),
    Dense(4, activation='relu'),
    Dense(1, activation='linear')  # Linear activation for regression
])

# Define the desired learning rate (e.g., 0.01)
learning_rate = 0.01

# Compile the model with the specified learning rate and mean_squared_error loss
optimizer = Adam(learning_rate=learning_rate)
model.compile(
    optimizer=optimizer,
    loss='mean_squared_error',  # Change to regression loss
    metrics=[
        'mae',  # Mean Absolute Error
        'mse'   # Mean Squared Error
    ]
)
# Display a summary of the model architecture
model.summary()

# Train the model
history = model.fit(X_train_final, y_train, epochs=8, batch_size=32, validation_split=0.1)

# Evaluate the model on the test set
loss, mae, mse = model.evaluate(X_test_final, y_test)
print("Mean Squared Error on Test Set:", mse)
print("Mean Absolute Error on Test Set:", mae)
print("Root Mean Squared Error on Test Set:", np.sqrt(mse))

end = time.time()
print(f"Computational time {end - start} seconds")

import tensorflow as tf
from tensorflow.keras.applications import ResNet101
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
start = time.time()

base_model = ResNet101(weights='imagenet', include_top=False, input_shape=(75, 75, 3))
base_model.trainable = False
# Build the regression model on top of the base model
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(16, activation='relu'),
    Dense(4, activation='relu'),
    Dense(1, activation='linear')  # Linear activation for regression
])

# Define the desired learning rate (e.g., 0.01)
learning_rate = 0.01

# Compile the model with the specified learning rate and mean_squared_error loss
optimizer = Adam(learning_rate=learning_rate)
model.compile(
    optimizer=optimizer,
    loss='mean_squared_error',  # Change to regression loss
    metrics=[
        'mae',  # Mean Absolute Error
        'mse'   # Mean Squared Error
    ]
)
# Display a summary of the model architecture
model.summary()

# Train the model
history = model.fit(X_train_final, y_train, epochs=8, batch_size=32, validation_split=0.1)

# Evaluate the model on the test set
loss, mae, mse = model.evaluate(X_test_final, y_test)
print("Mean Squared Error on Test Set:", mse)
print("Mean Absolute Error on Test Set:", mae)
print("Root Mean Squared Error on Test Set:", np.sqrt(mse))

end = time.time()
print(f"Computational time {end - start} seconds")

import tensorflow as tf
from tensorflow.keras.applications import ResNet152
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
start = time.time()

base_model = ResNet152(weights='imagenet', include_top=False, input_shape=(75, 75, 3))
base_model.trainable = False
# Build the regression model on top of the base model
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(16, activation='relu'),
    Dense(4, activation='relu'),
    Dense(1, activation='linear')  # Linear activation for regression
])

# Define the desired learning rate (e.g., 0.01)
learning_rate = 0.01

# Compile the model with the specified learning rate and mean_squared_error loss
optimizer = Adam(learning_rate=learning_rate)
model.compile(
    optimizer=optimizer,
    loss='mean_squared_error',  # Change to regression loss
    metrics=[
        'mae',  # Mean Absolute Error
        'mse'   # Mean Squared Error
    ]
)
# Display a summary of the model architecture
model.summary()

# Train the model
history = model.fit(X_train_final, y_train, epochs=8, batch_size=32, validation_split=0.1)

# Evaluate the model on the test set
loss, mae, mse = model.evaluate(X_test_final, y_test)
print("Mean Squared Error on Test Set:", mse)
print("Mean Absolute Error on Test Set:", mae)
print("Root Mean Squared Error on Test Set:", np.sqrt(mse))

end = time.time()
print(f"Computational time {end - start} seconds")

import tensorflow as tf
from tensorflow.keras.applications import EfficientNetB1
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
start = time.time()

base_model = EfficientNetB1(weights='imagenet', include_top=False, input_shape=(75, 75, 3))
base_model.trainable = False
# Build the regression model on top of the base model
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(16, activation='relu'),
    Dense(4, activation='relu'),
    Dense(1, activation='linear')  # Linear activation for regression
])

# Define the desired learning rate (e.g., 0.01)
learning_rate = 0.01

# Compile the model with the specified learning rate and mean_squared_error loss
optimizer = Adam(learning_rate=learning_rate)
model.compile(
    optimizer=optimizer,
    loss='mean_squared_error',  # Change to regression loss
    metrics=[
        'mae',  # Mean Absolute Error
        'mse'   # Mean Squared Error
    ]
)
# Display a summary of the model architecture
model.summary()

# Train the model
history = model.fit(X_train_final, y_train, epochs=8, batch_size=32, validation_split=0.1)

# Evaluate the model on the test set
loss, mae, mse = model.evaluate(X_test_final, y_test)
print("Mean Squared Error on Test Set:", mse)
print("Mean Absolute Error on Test Set:", mae)
print("Root Mean Squared Error on Test Set:", np.sqrt(mse))

end = time.time()
print(f"Computational time {end - start} seconds")

import tensorflow as tf
from tensorflow.keras.applications import MobileNetV3Small
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
start = time.time()

base_model = MobileNetV3Small(weights='imagenet', include_top=False, input_shape=(75, 75, 3))
base_model.trainable = False
# Build the regression model on top of the base model
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(16, activation='relu'),
    Dense(4, activation='relu'),
    Dense(1, activation='linear')  # Linear activation for regression
])

# Define the desired learning rate (e.g., 0.01)
learning_rate = 0.01

# Compile the model with the specified learning rate and mean_squared_error loss
optimizer = Adam(learning_rate=learning_rate)
model.compile(
    optimizer=optimizer,
    loss='mean_squared_error',  # Change to regression loss
    metrics=[
        'mae',  # Mean Absolute Error
        'mse'   # Mean Squared Error
    ]
)
# Display a summary of the model architecture
model.summary()

# Train the model
history = model.fit(X_train_final, y_train, epochs=8, batch_size=32, validation_split=0.1)

# Evaluate the model on the test set
loss, mae, mse = model.evaluate(X_test_final, y_test)
print("Mean Squared Error on Test Set:", mse)
print("Mean Absolute Error on Test Set:", mae)
print("Root Mean Squared Error on Test Set:", np.sqrt(mse))

end = time.time()
print(f"Computational time {end - start} seconds")
