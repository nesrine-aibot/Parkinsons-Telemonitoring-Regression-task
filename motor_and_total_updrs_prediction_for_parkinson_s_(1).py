# -*- coding: utf-8 -*-
"""motor-and-total-updrs-prediction-for-parkinson-s (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sYhddELQnoniB_g4zPo5KTS5RVutnyBY
"""

import numpy as np
import pandas as pd

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

import pandas as pd

file_path = r"parkinsons_updrs.data"
df = pd.read_csv(file_path)
print(df.head())

df.head()

df.shape

df.isnull().sum()

df.duplicated().sum()

df.info()

df.describe()

df.columns



### Heatmap - Correlation Matrix
plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor

# Initialize the LabelEncoder
le = LabelEncoder()

# Convert 'sex' column to numerical values (assuming 'False' and 'True' are categories)
df['sex'] = le.fit_transform(df['sex'])

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Step 5: Define Features and Target
X = df.drop(columns=["total_UPDRS"])
y = df["total_UPDRS"]

# Step 6: First split: 80% train+val and 20% test
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Step 7: Second split: 75% train, 25% val from the 80%
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=0.25, random_state=42
)

# Step 8: Feature Scaling (fit only on training)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# ✅ Optional: convert to DataFrame for easier inspection
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)
X_val_scaled = pd.DataFrame(X_val_scaled, columns=X.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)

# Final shapes
print("Train:", X_train_scaled.shape)
print("Val:", X_val_scaled.shape)
print("Test:", X_test_scaled.shape)

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Apply PCA to reduce features to 2D
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_train_scaled)

# Create scatter plot with colorbar
plt.figure(figsize=(10, 7))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train, cmap='viridis')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA Projection Colored by total_UPDRS')
cbar = plt.colorbar(scatter)
cbar.set_label('total_UPDRS')
plt.show()

# Clean feature names before fitting
X_train.columns = X_train.columns.str.replace(r'[^a-zA-Z0-9_]', '_', regex=True)
X_val.columns = X_val.columns.str.replace(r'[^a-zA-Z0-9_]', '_', regex=True)

# Then proceed with fitting the model
search.fit(X_train, y_train)

# Check the columns of X_test to see if they have any invalid characters
print(X_test.columns)

# Rename columns to remove special characters or spaces
X_test.columns = X_test.columns.str.replace(r'\W', '_', regex=True)  # Replaces non-word characters with underscores

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Apply PCA to reduce features to 2D
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_train_scaled)

# Create scatter plot with colorbar
plt.figure(figsize=(10, 7))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train, cmap='viridis')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA Projection Colored by total_UPDRS')
cbar = plt.colorbar(scatter)
cbar.set_label('total_UPDRS')
plt.show()

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# Example: Define hyperparameter grids for each model
param_grids = {
    "Linear Regression": {},  # No hyperparameters to tune
    "Ridge Regression": {"alpha": [0.01, 0.1, 1.0, 10.0]},
    "Lasso Regression": {"alpha": [0.001, 0.01, 0.1, 1.0]},
    "ElasticNet": {"alpha": [0.001, 0.01, 0.1, 1.0], "l1_ratio": [0.1, 0.5, 0.9]},
    "Decision Tree": {"max_depth": [5, 10, 20, None]},
    "Random Forest": {"n_estimators": [50, 100], "max_depth": [5, 10, None]},
    "Gradient Boosting": {"n_estimators": [100, 200], "learning_rate": [0.01, 0.1], "max_depth": [3, 5]},
    "AdaBoost": {"n_estimators": [50, 100], "learning_rate": [0.01, 0.1, 1.0]},
    "XGBoost": {"n_estimators": [100, 200], "learning_rate": [0.01, 0.1], "max_depth": [3, 6]},
    "LightGBM": {"n_estimators": [100, 200], "learning_rate": [0.01, 0.1], "num_leaves": [31, 50]}
}

results = {}
best_models = {}

# Loop through each model and run grid search on the training+validation data
for name, model in models.items():
    print(f"⏳ Tuning {name}...")
    param_grid = param_grids.get(name, {})

    if param_grid:
        search = GridSearchCV(model, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)
    else:
        search = model  # No tuning needed

    if isinstance(search, GridSearchCV):
        search.fit(X_train, y_train)
        best_model = search.best_estimator_
    else:
        search.fit(X_train, y_train)
        best_model = search

    # Evaluate on validation set
    y_pred = best_model.predict(X_val)
    rmse = np.sqrt(mean_squared_error(y_val, y_pred))
    r2 = r2_score(y_val, y_pred)
    mae = mean_absolute_error(y_val, y_pred)

    results[name] = {"RMSE": rmse, "R2 Score": r2, "MAE": mae}
    best_models[name] = best_model

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Apply PCA to reduce features to 2D
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_train_scaled)

# Create scatter plot with colorbar
plt.figure(figsize=(10, 7))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train, cmap='viridis')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA Projection Colored by total_UPDRS')
cbar = plt.colorbar(scatter)
cbar.set_label('total_UPDRS')
plt.show()

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import numpy as np


for name, best_model in best_models.items():
    print(f"⏳ Evaluating {name} on Test Set...")

    # Make predictions on the test set
    y_pred_test = best_model.predict(X_test)

    # Calculate the evaluation metrics
    r2_test = r2_score(y_test, y_pred_test)
    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))
    mae_test = mean_absolute_error(y_test, y_pred_test)

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Apply PCA to reduce features to 2D
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_train_scaled)

# Create scatter plot with colorbar
plt.figure(figsize=(10, 7))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train, cmap='viridis')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA Projection Colored by total_UPDRS')
cbar = plt.colorbar(scatter)
cbar.set_label('total_UPDRS')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Convert results to DataFrame
results_df = pd.DataFrame(results).T  # Convert the 'results' dictionary to a DataFrame
print(results_df)

# Assuming the column name for R² Score is "R2 Score"
plt.figure(figsize=(10, 5))
sns.barplot(data=results_df, x=results_df.index, y="R2 Score")  # Adjusted column name
plt.xticks(rotation=45)
plt.title("Model R² Score Comparison on Validation Set")
plt.ylabel("R² Score")
plt.xlabel("Model")
plt.show()

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Apply PCA to reduce features to 2D
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_train_scaled)

# Create scatter plot with colorbar
plt.figure(figsize=(10, 7))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train, cmap='viridis')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA Projection Colored by total_UPDRS')
cbar = plt.colorbar(scatter)
cbar.set_label('total_UPDRS')
plt.show()

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import numpy as np

# Combine X_test with actual y_test for PCA
combined_actual = np.hstack((X_test, y_test.values.reshape(-1, 1)))
pca_actual = PCA(n_components=2)
X_pca_actual = pca_actual.fit_transform(combined_actual)

# Combine X_test with predicted y values
combined_pred = np.hstack((X_test, y_pred_test.reshape(-1, 1)))
pca_pred = PCA(n_components=2)
X_pca_pred = pca_pred.fit_transform(combined_pred)

# Create subplots
fig, axs = plt.subplots(1, 2, figsize=(14, 6))

# Plot 1: PCA with actual labels
sc1 = axs[0].scatter(X_pca_actual[:, 0], X_pca_actual[:, 1], c=y_test, cmap='viridis')
axs[0].set_title("PCA - Colored by Actual total_UPDRS")
axs[0].set_xlabel("Principal Component 1")
axs[0].set_ylabel("Principal Component 2")
cbar1 = plt.colorbar(sc1, ax=axs[0])
cbar1.set_label("Actual total_UPDRS")

# Plot 2: PCA with predicted labels
sc2 = axs[1].scatter(X_pca_pred[:, 0], X_pca_pred[:, 1], c=y_pred_test, cmap='viridis')
axs[1].set_title("PCA - Colored by Predicted total_UPDRS")
axs[1].set_xlabel("Principal Component 1")
axs[1].set_ylabel("Principal Component 2")
cbar2 = plt.colorbar(sc2, ax=axs[1])
cbar2.set_label("Predicted total_UPDRS")

plt.tight_layout()
plt.show()

pca = PCA(n_components=2)
X_pca = pca.fit_transform(y_test + y_pred_test)

# Create scatter plot with colorbar
plt.figure(figsize=(10, 7))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train, cmap='viridis')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA Projection Colored by total_UPDRS')
cbar = plt.colorbar(scatter)
cbar.set_label('total_UPDRS')
plt.show()

import matplotlib.pyplot as plt

# Create a 1-row, 2-column figure
fig, axs = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns

# First plot
axs[0].plot([1, 2, 3], [4, 5, 6])
axs[0].set_title("Plot 1")
axs[0].set_xlabel("X")
axs[0].set_ylabel("Y")

# Second plot
axs[1].plot([1, 2, 3], [6, 5, 4])
axs[1].set_title("Plot 2")
axs[1].set_xlabel("X")
axs[1].set_ylabel("Y")

# Adjust layout
plt.tight_layout()
plt.show()